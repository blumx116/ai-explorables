---
template: post_v2.html
title: Dissecting the Mamba
title: Understanding Mamba Models via Mechanistic Interpretability
socialsummary: An interactive exploration of memory in SSMs.
shareimg: https://pair.withgoogle.com/explorables/images/grokking.png # TODO: make a picture
shareimgabstract: https://pair.withgoogle.com/explorables/images/grokking-abstract.png # TODO: make a picture
authors: Carter Blum
date: April 2024
permalink: /ssm-attn/
---


Transformers, the architecture behind many recent breakthroughs in AI (including Gemini, ChatGPT, and Claude), have a fundamental flaw.
While they have shown incredible abilities on many tasks, the amount of compute and that they require scales quadratically with the length of the sequence that they're processing.
This doesn't pose problems when you're just asking Gemini a quick question, but it becomes a serious limitation if you want your model to have long term memory or reason over large pieces of text.

State Space Models (SSMs) are a classical type of sequence model that has received dramatically increased attention recently, owing in large part to their performance on very long sequences <a class="citestart" key="s4"></a>. 
In contrast to transformers, the computational resources required to run them don't explode as you get to very long sequences.
And, unlike the more traditional Recurrent Neural Networks (RNNs), they can be trained efficiently and are able to effectively retain information in their context.
There are some excellent resources on [the theory behind how SSMs work](https://srush.github.io/annotated-s4/), [how they are able to run so quickly](https://srush.github.io/annotated-mamba/hard.html) and [even on how they differ from traditional recurrent models](https://www.youtube.com/watch?v=N6Piou4oYx8).
However, when you look at the state of the art SSMs, like Mamba<a class="citestart" key="mamba"></a>, you'll notice that the actual SSM is only a small part of each layer!

<center><img src="img/ssm-attn-layer.png" /></center>

In this article, we'll examine a toy Mamba model through the lense of mechanistic interpretability.
By examining how each of the components above work together to solve an example problem, we aim to build a stronger intuition about why the models are designed the way they are.
We'll mostly be interested in how the various pieces of the Mamba layer work together, but having a basic understanding of the SSM component will be useful for that, so we'll provide a brief recap of them below.

# Review of State Space Models

State space models actually have a very long legacy, dating back to the 1960s <a class="citestart" key="kalman"></a>.
They attempt to model one sequence `$y_t$`, as a function of another, `$u_t$`.
For instance, if you wanted to model filling up a bucket of water, `$u_t$` might be whether or not the faucet is on at time `$t$` and `$y_t$` might be whether or not the bucket is overflowing with water at time `$t$`.
It's probably apparent that knowing whether or not the faucet is on *right now* doesn't tell you whether or not the bucket is overflowing *right now* - maybe you just turned the faucet on and the bucket is empty!
You need to keep track of how full the bucket is, which means you need some kind of state (typically, `$h_t$`) - this is where state space models get their name from.

At its core, this is really all that a classical SSM does: it uses some input to track a state over time, and then uses that state to output some value.
One classic form of this is as follows:

$$h_t' = Ah_t + Bu_t$$
$$y_t = Cy_t + Du_t$$

To create a state space model, we're trying to find good parameters for `$A$`, `$B$`, `$C$`, and `$D$`.
On a high level, you can think of each matrix as playing the following role:
 - `$A$` determines how the state evolves over time, in the absence of input
 - `$B$` governs how the information from `$u_t$` affects the state `$h_t$`
 - `$C$` models how the state affects the output
 - `$D$` lets the model bypass the intermediate state and have the input `$u_t$`, directly affect the output `$y_t$`


However, you may have noticed that the above equation is continuous, whereas many important tasks, including language modeling, are discrete.
As a result, we need to discretize the above equations.
For a timestep of size `$\Delta$`, we need instead write:

$$h_t = \bar{A}h_{t-\Delta} + \bar{B}u_t$$
$$y_t = Ch_t + Du_t$$

Where `$\bar{A}$` and `$\bar{B}$` are discretized versions of the `$A$` and `$B$` matrices.
Mamba uses a simplified version of [Zero-Order Hold discretization](https://en.wikipedia.org/wiki/Zero-order_hold), where

$$\bar{A} = exp(\Delta A)$$
$$\bar{B} = \Delta B$$

You might notice that when `$\Delta$` is nearly 0, then `$\bar{A} \approx I$` and `$\bar{B} \approx 0$`.
Intuitively, in this case, `$h_t \approx h_{t-\Delta}$`.
In simple English, when we only increment by a small timestep, our hidden state barely changes.
The reverse is also true.

Because the `$A$` matrix in Mamba is set up to ensure that `$\bar{A} \rightarrow 0$` as `$\Delta$` grows <a class='footstart' key="A-restrictions"></a>, high values of `$\Delta$` result in the model 'forgetting' its state.
At the same time, `$\bar{B}$` grows with `$\Delta$`, which means that larger values of `$\Delta$` will result in `$h_t$` mostly ignoring `$h_{t-\Delta}$` and instead primarily being a function of `$u_t$`.

Mamba and other state space models allow the model to dynamically adjust the value of `$\Delta$` for each element in a sequence.
This enables the models to select when to keep their existing memory and when to instead update based on new observations.
We'll see this ability become relevant again below.

# Task

To illustrate how the parts of an SSM work together, we'll examine a toy model using a simple task related to associative recall <a class="citestart" key="AR"></a><a class="citestart" key="zoology"></a>.
Associative recall has been theorized to be crucial for the impressive in-context learning that large language models display.
At its core, it asks the model to do something very simple - when it sees a particular token, it 'looks back' in its context to see what happened last time that token was in its context.
For instance, if the model is given the phrase "Jeff Dean is the lead scientist at Google Research", the next time it outputs "Jeff", it may look back to the context to remind itself that "Dean" is his last name.<a class='footstart' key="jeff-dean"></a>
In transformers, this ability has been observed to be implemented by special attention heads called induction heads<a class="citestart" key="inductionheads"></a>.

For ease of analysis, we'll examine a simplified version of this task.
We'll train our model on a sequence to sequence task where the model is fed a mostly-random sequence of tokens.
Most of the time, it should just act as the identity function, simply outputting whatever token is input to it.
*However*, when it sees the special recall token, ✻, it should remember the token that comes after it and instead output *that* next time it sees the special recall token.

For those who are accustomed to python, the model should learn to implement the functionality below.
<br>
<br>


```python
class Model:
   def __init__(self):
       self.prev_token: Optional[str] = None
       self.memory: Optional[str] = None

   def __call__(self, token: str) -> str:
       if self.prev_token == "✻":
           self.memory = token
       self.prev_token = token
       if token == "✻" and self.memory is not None:
           return self.memory
       else:
           return token
```

<br>


For example, if the model sees the sequence "A✻B✻", it should output "A✻BB", because the second ✻ should be replaced with "B", which came after the first instance of ✻. The rest of the characters should be the same as the input sequence.
For a somewhat more complicated example, given the input sequence "ABC✻ABC✻ABC", the model should output "ABC✻ABCAABC".

Below, we'll examine a simple Mamba model that has learned to solve this task.
To keep things visually interpretable, we'll restrict ourselves to just these 4 tokens: ('A', 'B', 'C' & '✻') and only look at short sequences.


# Model

For this task, we'll employ a very simple model.
Following the work in <a class="citestart" key="transformer-circuits"></a>, we'll limit our model to a single layer and remove layer norms from the model.
Removing layer norms shouldn't affect model expressiveness, but can make interpreting the outputs substantially easier - if you repeat this experiment with layer norms turned on, the mamba layer will often output near-zero values and rely on the layer norm to blow them up to scales that affect the output logits.

Below is a complete diagram of our model.

<center><img src="img/full-network.png" onload="this.width/=2;this.onload=null;"/></center>

The embeddings, unembeddings and residual connection are all fairly standard in language modeling, so we won't be discussing them here.

# Model Exploration

## The Role of `$\Delta$`

In our review of selective SSMs above, we noted that `$\Delta$` can act as a gating mechanism for the model to control access to its memory.
When `$\Delta$` is high, the model's state evolves more rapidly and inputs have a large effect on the state.
Conversely, when `$\Delta$` is low, the model can preserve its state, allowing it to be undisturbed by inputs.
In the task we outlined above, the model only really needs to remember one thing: the token that comes after ✻.
We then might reasonably hypothesize that the model will have a higher value of `$\Delta$` on timesteps around ✻.

You can play around with the input below to see how `$\Delta$` responds to the input sequence.

<center><input id="input-box" type="text" onkeydown="return /[ABC\*]/i.test(event.key)" onkeyup="input_box_callback()" value="ABC✻ABC✻ABC" maxlen="10"></center>
<center><div id="dt-visualization"></div></center>

As predicted, `$\Delta$` spikes exactly one timestep after the special ✻ token, when we would expect it to be writing to memory <a class="footstart" key="delay2"></a>.
But how does the model create a delayed spike in `$\Delta$` *after* seeing the special token?
You might notice that, in a one-layer network, we calculate `$\Delta$` without ever having 'read from' the hidden state, because `$\Delta$` is an input to the SSM equation, not an output<a class="footstart" key="fixed-delay"></a>. The model needs a way to change its behaviour based on recent inputs without using the hidden state. 
In this case, it uses the 1D convolution module to implement this.

## How 1D Convolution Ties In

Almost all of the inputs to the core SSM module are based on the output of Mamba's 1D convolution module.
In the SSM equations above, `$u_t$` is the output of the 1D convolution module and `$\Delta$`, `$B$` and `$C$` are all computed as linear projections of `$u_t$` <a class="footstart" key="BC-dynamic"></a>.

We've seen that the model is clearly embedding some information about when we are one timestep after the special ✻ token.
Because this model is so small, we can visually inspect the weights to look for clues about how it is doing this.

<center><div id="conv-heatmap"></div></center>

Before we analyze the above, it's useful to review how to interpret these weights.
Going from left to right, we have the kernel dimension, applied over time.
The rightmost values are multiplied by the inputs of the current timestep, while the leftmost values are multiplied by the inputs from 3 timesteps ago.
While the full explanation of the vertical axis is a little bit more nuanced, it's sufficient to think of it as the hidden dimension for our purposes.
Each row is convolved independently with a separate dimension of the projected input.

Turning back to the weights themselves, as we would anticipate, the values one the left are nearly 0.
The hidden state is keeping track of the long-term memory, and there's no other reason for the model to pay attention to what happened more than 1 timestep ago, so these values have essentially no effect on the convolution.

We additionally notice that the weights respond very strongly to the 1 timestep in the past, particularly along the middle row.
This looks promising, so we can take a peak and see if we notice anything similar in the inputs to the convolution module.

<center><div id="conv-input-heatmap"></div></center>

Sure enough, we notice correspondingly high values in the middle row for the special token!
So, it looks like we've established that the model uses this dimension to encode information about when to update the memory.
This is unsurprisingly a very important part of the model's functionality. 
If we plot the representations of each of the tokens over time, we can see that the model very quickly learn to separate the special ✻ token from the other tokens along this dimension.

<center><div id="plot-over-time"></div></center>

# What About the Multiplication At The End

While we've covered most of the main Mamba pipeline, you may notice that there's a second, smaller pipeline that gets multiplied with the output of the SSM.
This pipeline is responsible for gating the output of the SSM module. 
Intuitively, it looks at the input of the layer and 'decides' if the output of the SSM will be passed to the later layers of the SSM.
When might this be useful?

Recall that in our example task, the model should act as the identity function *most* of the time. 
The memory of the SSM is only relevant when it sees the special ✻ token for the second time, forcing it to output the token that it stored earlier.
The rest of the time, the output of memory is just noise.

Before we look for evidence of how the gate behaves in our task, we should understand at a basic level how it's implemented.
Mathematically, the gating mechanism is implemented by multiplying the outputs of each SSM sequence by a value chosen by the gate.
If the gate outputs a value of 0, this completely supresses the output of that particular SSM sequence (whereas a high positive value will amplify it).
By default, Mamba uses [SiLU](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/activation_functions.html#flax.linen.activation.silu) to rescale the outputs of the gate, making it very easy for the model to suppress outputs because most negative values cause SiLU to output 0.

We then expect that the gate should look pretty similar to `$\Delta$`, where it spikes when it sees the special ✻ token.
However, unlike `$\Delta$`, we expect it to spike on exactly the timestep where we see the special ✻ token, whereas the `$\Delta$` function was delayed by a timestep <a class="footnote" key="gate-always"></a>.

<center><div id="gate-heatmap"></div></center>

As we can see, the model learns a gate that nearly completely supresses the output of the SSM module at all timesteps except the timestep where the special ✻ token occurs.
This helps the model only 'access' its memory on timesteps where it can aid its prediction, and otherwise use the memory residual connection to ignore the SSM layer <a class="footnote" key="mostly-ignore"></a>.
Recall that each Mamba layer processes multiple timeseries separately, and each of these timeseries can be turned on or off by gates individually. 
For simplicity, we only showed one such gate above.

# Tying it all together

Hopefully you now have a better idea of what all of the additional parts of a Mamba layer are doing.
While the core of the SSM is fairly traditional, the authors have made a number of modifications to shore up traditional shortcomings of these models.

The `$\Delta$` parameter allows the model to selectively recall and ignore information at various timesteps.
Our model used this ability to allow it to selectively remember the tokens that come after the special ✻ token.

The 1D convolution that comes before the SSM allows the model to create behaviour that triggers a fixed number of tokens away from each other.
Without the 1D convolution, we would be able to spike `$\Delta$` when we see the special ✻ token, but it would be very difficult to do so with a 1 timestep delay.

Finally, the multiplication after the SSM acts as a gating mechanism on the influence of the memory.
This allows the model to separate control whether or not to 'read' from the memory, even if it may have 'written' to memory at that timestep.
In our task, this allowed the model to to ignore its state on timesteps where it was irrelevant.

Each of these changes serves to fix observed issues that SSMs encounter while preserving the important performance characteristics that are driving interest in this class of models.
Hopefully now all of those steps no longer look like a random sequence of operations, and instead are imbued with intuition about why they're each there.

# Open Questions (Draft Only)
 - Why is a nonlinearity applied to the output of 1D convolution?
 - Why is the same `$D$` matrix applied to every SSM series?
 - What is the point of the output matrix projection at the end of the layer (besides adjusting dimensionality)? Couldn't it be folded in to `$C$` and `$D$`?
 - The expressivity of `$A$` seems incredibly limited, is there any intuition arguing why this isn't a performance bottleneck?
 - I'm only showing the gate of 1 of 3 SSM dimensions at the moment, because the other two do something completely different. I should figure out what they're doing.


## Footnotes

<a class="footend" key="A-restrictions"></a> 
So that the hidden state doesn't blow up over time, Mamba restricts A to be a diagonal matrix full of negative values, which in turn means that `$exp(\Delta A)$` has eigenvalues that are always less than 1 and decrease towards 0 as `$\Delta$` decreases towards 0. Mamba implements this requirement by learning a different variable, `$A_{log}$`, and computing `$A := -exp(A_{log})$` each forward pass.

<a class="footend" key="jeff-dean"></a>
Jeff Dean may be famous enough that the model has memorized his name within its weights, so you might not always observe this behaviour on this specific example.
However, the general principle still holds.

<a class="footend" key="fixed-delay"></a>
Even if we were using a multi-layer network, it's actually very hard to create behavior that is delayed by a fixed number of timesteps using Mamba's formulation of an SSM.
There are a number of reasons for this, but the most pertinent is that Mamba restricts the `$A$` matrix to be diagonal, whereas you would need an off-diagonal `$A$` matrix to implement this<a class="citestart" key="AR"></a>.

<a class="footend" key="delay2"></a>
The way that the task is formulated, the model will never actually need to use the memory of what it saw until at least 2 timesteps after the special token.
As a result, the model sometimes converges to a solution where the peak in `$\Delta$` occurs two timesteps after the special ✻ token.
This ends implementing the rest of the functionality in a nearly identical way, but is somewhat less intuitive, so we've chosen not to highlight it here.

<a class="footend" key="BC-dynamic"></a>
We've seen compelling reasons for `$\Delta$` to be dynamically computed based on `$u_t$` above.
Unfortunately, there isn't an obviously compelling argument for `$B$` and `$C$` to be dynamically calculated, the authors of the Mamba paper just note that allowing them to be dynamically computed yields better modeling performance.
<a class="footend" key="gate-always"></a>
Ideally, the gating function wouldn't open the first time we see the special ✻ token, because the model doesn't need to fetch anything from memory in that case.
However, we have a single layer model and the gating function is just a linear projection at each timestep, so it can only look at tokens in isolation and either open for all instances of a token or none of them.
Luckily, this isn't a huge problem because there "isn't anything in memory" at the time we see the first intance of special ✻ token, so it doesn't impact anything too heavily (the SSM doesn't have any biases).

## Citations

<a class="citeend" key="transformer-circuits"></a>Elhage, et al., "A Mathematical Framework for Transformer Circuits", Transformer Circuits Thread, 2021.

<a class="citeend" key="zoology"></a>Eyuboglu, "Measuring and Improving Recall in Efficient Language Models", Hazy Research Blog, 2023

<a class="citeend" key="AR"></a>Fu, "Hungry Hungry Hippos: Towards Language Modeling With State Space Models", ICLR, 2023.

<a class="citeend" key="s4"></a>Gu, et al., "Efficiently Modeling Long Sequences with Structured State Spaces", ICLR, 2022.

<a class="citeend" key="mamba"></a>Gu, et al. "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", ArXiV, 2023

<a class="citeend" key="kalman"></a>Kalman, "A new approach to linear filtering and prediction problems." Transactions of the ASME--Journal of Basic Engineering, 82:35-45, 1960.

<a class="citeend" key="inductionheads"></a>Olsson, et al., "In-context Learning and Induction Heads", Transformer Circuits Thread, 2022.





<script id='MathJax-script' async src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'></script>
<script defer src='https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/mathtex-script-type.min.js' integrity='sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT' crossorigin='anonymous'></script>
<script src="https://cdn.plot.ly/plotly-2.32.0.min.js" charset="utf-8"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">

<script src='../third_party/d3_.js'></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.18.0/dist/tf.min.js"></script>

<script src="../third_party/jqueryv3.6.1.js"></script>
<script src="../third_party/citation_v2.js"></script>
<script src='../third_party/footnote_v2.js'></script>
<link rel='stylesheet' href='../third_party/footnote_v2.css'>
<link rel='stylesheet' href='../third_party/citation_v2.css'>
<link rel='stylesheet' href='style.css'>


<script src="ssm_impl_native.js"></script>
<script src="constants.js"></script>
<script src="visualize_dt_native.js"></script>
<script src="visualize_weights_plotly.js"></script>
<script src="embeddings_over_time2.js"></script>
<script src="init.js"></script>

<link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css"
      integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc"
      crossOrigin="anonymous"
    />
