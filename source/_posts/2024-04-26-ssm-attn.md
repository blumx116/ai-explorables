---
template: post_v2.html
title: Dissecting the Mamba
title: Understanding Mamba Models via Mechanistic Interpretability
socialsummary: An interactive exploration of memory in SSMs.
shareimg: https://pair.withgoogle.com/explorables/images/grokking.png # TODO: make a picture
shareimgabstract: https://pair.withgoogle.com/explorables/images/grokking-abstract.png # TODO: make a picture
authors: Carter Blum
date: April 2024
permalink: /ssm-attn/
---


Transformers, the architecture behind many recent breakthroughs in AI (including Gemini, ChatGPT, and Claude), have a fundamental flaw.
While they have shown incredible abilities on many tasks, the amount of compute and memory<a class="footstart"></a> that they require scales quadratically with the length of the sequence that they're processing.
This doesn't pose problems when you're just asking Gemini a quick question, but it becomes a serious limitation if you want your model to have long term memory or reason over large pieces of text.

State Space Models (SSMs) are a classical type of sequence model that has received dramatically increased attention recently, owing in large part to [their performance on very long sequences](https://arxiv.org/abs/2111.00396). 
In contrast to transformers, the computational resources required to run them don't explode as you get to very long sequences.
And, unlike the more traditional Recurrent Neural Networks (RNNs), they can be trained quickly and are able to effectively retain information in their context.
There are some excellent resources on [the theory behind how SSMs work](https://srush.github.io/annotated-s4/) and [how they are able to run so quickly](https://srush.github.io/annotated-mamba/hard.html).
However, when you look at the state of the art SSMs, like [Mamba](https://arxiv.org/abs/2312.00752), you'll notice that the actual SSM is only a small part of each layer!

<center><img src="img/ssm-attn-layer.png" /></center>

In this article, we'll examine a toy model through the lense of mechanistic interpretability.
By examining how each of the components above work together to solve a useful problem, we aim to build a stronger intuition about why the models are designed the way they are.
We'll mostly be interested in how the various pieces of the Mamba layer work together, but having a basic understanding of the SSM component will be useful for that, so we'll provide a brief recap of them below.

# Review of State Space Models

State space models actually have a very long legacy, dating back to the 1960s <a class="citestart" key="kalman"></a>.
They attempt to model one sequence `$y_t$`, as a function of another, `$u_t$`.
For instance, if you wanted to model filling up a bucket of water, `$u_t$` might be whether or not the faucet is on at time `$t$` and `$y_t$` might be whether or not the bucket is overflowing with water at time `$t$`.
It's probably apparent that knowing whether or not the faucet is on *right now* doesn't tell you whether or not the bucket is overflowing *right now* - maybe you just turned the faucet on and the bucket is empty!
You need to keep track of how full the bucket is, which means you need some kind of state (typically, `$h_t$`) - this is where state space models get their name from.

At its core, this is really all that a classical SSM does: it uses some input to track a state over time, and then uses that state to output some value.
One classic form of this is as follows:

$$h_t' = Ah_t + Bu_t$$
$$y_t = Cy_t + Du_t$$

To create a state space model, we're trying to find good parameters for `$A$`, `$B$`, `$C$`, and `$D$`.
On a high level, you can think of each matrix as playing the following role:
 - `$A$` determines how the state evolves over time, in the absence of input.
 - `$B$` is responsible for updating the state with the new information from `$u_t$`
 - `$C$` models how the state affects the output
 - `$D$` lets the model bypass the intermediate state and have the input `$u_t$`, directly affect the output `$y_t$`.

However, you may have noticed that the above equation is continuous, whereas many important tasks, including language modeling, are discrete.
As a result, we need to discretize the above equations.
For a timestep of size `$\Delta$`, we need instead write:

$$h_t = \bar{A}h_{t-\Delta} + \bar{B}u_t$$
$$y_t = Ch_t + Du_t$$

Where `$\bar{A}$` and `$\bar{B}$` are discretized versions of the `$A$` and `$B$` matrices.
Mamba uses a simplified version of Zero-Order Hold discretization <a class="citestart" key="ZOH"></a>, where

$$\bar{A} = exp(\Delta A)$$
$$\bar{B} = \Delta B$$

You might notice that when `$\Delta$` is nearly 0, then `$\bar{A} \approx I$` and `$\bar{B} \approx 0$`.
Intuitively, in this case, `$h_t \approx h_{t-\Delta}$`.
In simple English, when we only increment by a small timestep, our hidden state barely changes.
The reverse is also true.

Because the `$A$` matrix in Mamba is set up to ensure that `$\bar{A} \rightarrow 0$` as `$\Delta$` grows <a class='footstart'>¹</a>, high values of `$\Delta$` result in the model 'forgetting' its state.
At the same time, `$\bar{B}$` grows with `$\Delta$`, which means that larger values of `$\Delta$` will result in `$h_t$` mostly ignoring `$h_{t-\Delta}$` and instead primarily being a function of `$u_t$`.

Mamba and other state space models allow the model to dynamically adjust the value of `$\Delta$` for each element in a sequence.
This enables the models to select when to keep their existing memory and when to instead update based on new observations.
We'll see this ability become relevant again below.

# Task

To illustrate how the parts of an SSM work together, we'll examine a toy model using a simple task related to associative recall <a class="citestart" key="AR"></a><a class="citestart" key="zoology"></a>.
Associative recall has been theorized to be crucial for the impressive in-context learning that large language models display.
At its core, it asks the model to do something very simple - when it sees a particular token, it 'looks back' in its context to see what happened last time that token was in its context.
For instance, if the model is given the phrase "Jeff Dean is the lead scientist at Google Research", the next time it outputs "Jeff", it may look back to the context to remind itself that "Dean" is his last name.<a class='footstart'></a>
In transformers, this ability has been observed to be implemented by special attention heads called induction heads<a class="citestart" key="inductionheads"></a>.

For ease of analysis, we'll examine a simplified version of this problem.
We'll train our model on a sequence to sequence task, where the model is fed a mostly-random sequence of tokens.
Most of the time, it should just act as an identity function, simply outputting whatever token is input to it.
*However*, when it sees the special recall token, ✻, it should remember the token that comes after it and instead output *that* next time it sees the special recall token.

For those who are accustomed to python, the model should learn to implement the functionality below.
<br>
<br>


```python
class Model:
   def __init__(self):
       self.prev_token: Optional[str] = None
       self.memory: Optional[str] = None

   def __call__(self, token: str) -> str:
       if self.prev_token == "✻":
           self.memory = token
       self.prev_token = token
       if token == "✻" and self.memory is not None:
           return self.memory
       else:
           return token
```

<br>


For example, if the model sees the sequence "A✻B✻", it should output "A✻BB", because the second ✻ should be replaced with "B", which came after the first instance of ✻. The rest of the characters should be the same as the input sequence.
For a somewhat more complicated example, given the input sequence "ABC✻ABC✻ABC", the model should output "ABC✻ABCAABC".
To keep things visually interpretable, we'll restrict ourselves to just these 4 tokens: ('A', 'B', 'C' & '✻') and only look at short sequences.


# Model

For this task, we'll employ a very simple model.
Following the work in <a class="citestart" key="transformer-circuits"></a>, we'll limit our model to a single layer and remove layer norms from the model.
Removing layer norms shouldn't affect model expressiveness, but can make interpreting the outputs substantially easier - if you repeat this experiment with layer norms turned on, the mamba layer will often output near-zero values and rely on the layer norm to blow them up to scales that affect the output logits.

Below is a complete diagram of our model.

<center><img src="img/full-network.png" onload="this.width/=2;this.onload=null;"/></center>


# Model Exploration

## The role of `$\Delta$`

In our review of selective SSMs above, we noted that `$\Delta$` can act as a gating mechanism for the model to control access to its memory.
When `$\Delta$` is high, the model's state evolves more rapidly and inputs have a large effect on the state.
Conversely, when `$\Delta$` is low, the model can preserve its state, allowing it to be undisturbed by inputs.
In the task we outlined above, the model only really needs to remember one thing: the token that comes after ✻.
We then might reasonably hypothesize that the model will have a higher value of `$\Delta$` on timesteps around ✻.


<center><input id="input-box" type="text" onkeydown="return /[ABC\*]/i.test(event.key)" onkeyup="input_box_callback()" value="ABC✻ABC✻ABC"></center>
<center><div id="app"></div></center>

As predicted, the attention spikes right after the special token, exactly when we would expect it to be writing to memory.
Furthermore, they're happening exactly one timestep after the special ✻ token, exactly as we'd anticipate <a class="footstart" key="delay2"></a>.
Feel free to play around with the input above to convince yourself that the special token really does create these spikes in `$\Delta$`.
But how does the model create a delayed spike in `$\Delta$` *after* seeing the special token?
You might notice that, in a one-layer network, we calculate `$\Delta$` without ever having 'read from' the hidden state, because `$\Delta$` is an input to the SSM equation, not an output<a class="footstart"></a>.
Instead, we use 1D convolution to implement this functionality.

## The role of convolution

Almost all of the inputs to the core SSM module are based on the output of Mamba's 1D convolution module.
In the SSM equations above, `$u_t$` is the output of the 1D convolution module and `$\Delta$`, `$B$` and `$C$` are all computed as linear projections of `$u_t$` <a class="footstart" key="BC-dynamic"></a>.

We've seen that the model is clearly embedding some information about when we are one timestep after the special ✻ token.
Because this model is so small, we can visually inspect the weights to look for clues about how it is doing this.

<center><div id="conv-heatmap"></div></center>

Before we analyze the above, it's useful to review how to interpret these weights.
Going from left to right, we have the kernel dimension, applied over time.
The rightmost values are multiplied by the inputs of the current timestep, while the leftmost values are multiplied by the inputs from 3 timesteps ago.
While the full explanation of the vertical axis is a little bit more nuanced, it's sufficient to think of it as the hidden dimension for our purposes.
Each row is convolved independently with a separate dimension of the projected input.

Turning back to the weights themselves, as we would anticipate, the values one the left are nearly 0.
The hidden state is keeping track of the long-term memory, and there's no other reason for the model to pay attention to what happened more than 1 timestep ago, so these values have essentially no effect on the convolution.

We additionally notice that the weights respond very strongly to the 1 timestep in the past, particularly along the middle row.
This looks promising, so we can take a peak and see if we notice anything similar in the inputs to the convolution module.

<center><div id="conv-input-heatmap"></div></center>

Sure enough, we notice correspondingly high values in the middle row for the special token!
So, it looks like we've established that the model uses this dimension to encode information about when to update the memory.
This is unsurprisingly a very important part of the model's functionality. 
If we plot the representations of each of the tokens over time, we can see that the model very quickly learn to separate the special ✻ token from the other tokens along this dimension.

<center><div id="plot-over-time"></div></center>




<center><div id="app"></div></center>

<center><div id="heatmap"></div></center>
<center><div id="interactive-heatmap">
    <div id="interactive-left" style="color: red; float: left;">
        <div id="tooltip" ></div>
    </div>
    <div id="plotly-heatmap" style="float: right;"></div>
</div></center>

# Task

# SSM Implementation of Task

 - dt_rank
 - convolution
 - gate 

- Introduction
 - What are SSMs
    - Alternative/complement to transformers/RNNs
    - Recently received a large amount of interest
    - Used in recent releases including Jamba
 - Why do we care about SSMs
    - vs Transformers
        - Linear inference speed/memory 
    - vs RNNs
        - Parallelizable training
        - Long-term recall [cite S4]
 - Review of Mamba
    - Particularly performant version of SSM that has received attention
    - Modified from H3 model
    - Allows selective propagation/forgetting of information
    - Very complicated
 - Motivation for this post
    - Mamba can be difficult to grok
    - Motivated by selective copying as a means of compression
    - Induction Heads
- Task
 - 1 Sentence Blurb
  - Why this task is interesting
  - Task specifics
- SSM Implementation of Task
  - Model Details
   - Model Size
    - (footnote) why not smaller?
   - Simplifications
    - (footnote) training 
  - Sanity Checks
    - Embedding Representation
    - Appearance of special tokens
  - How does the model know when to save information?
  - How does the model know when to output from memory?
  - How does the model represent the other tokens?
- Summary
  - Role of dt
  - Role of convolution
  - Role of gate
- Appendix
  - Dimensionality in SSM
  - Quirks of SSM representation

<a class="footend"></a> 
So that the hidden state doesn't blow up over time, Mamba restricts A to be a diagonal matrix full of negative values, which in turn means that `$exp(\Delta A)$` has eigenvalues that 
 - Are always less than 1
 - Decrease towards 0 as $\Delta$ 
Mamba implements this requirement by learning a different variable, `$A_{log}$`, and computing `$A := -exp(A_{log})$` each forward pass.
<a class="footend"></a>
Jeff Dean may be famous enough that the model has memorized his name within its weights, so you might not always observe this behaviour on this specific example.
However, the general principle still holds.
<a class="footend"></a>
Even if we were using a multi-layer network, it's actually very hard to create behavior that is delayed by a fixed number of timesteps using Mamba's formulation of an SSM.
There are a number of reasons for this, but the most pertinent is that Mamba restricts the `$A$` matrix to be diagonal, whereas you would need an off-diagonal `$A$` matrix to implement this<a class="citestart" key="h3"></a>.
<a class="footend" key="delay2"></a>
The way that the task is formulated, the model will never actually need to use the memory of what it saw until at least 2 timesteps after the special token.
As a result, the model sometimes converges to a solution where the peak in `$\Delta$` occurs two timesteps after the special ✻ token.
This ends implementing the rest of the functionality in a nearly identical way, but is somewhat less intuitive, so we've chosen not to highlight it here.

<a class="footend" key="BC-dynamic"></a>
We've seen compelling reasons for `$\Delta$` to be dynamically computed based on `$u_t$` above.
Unfortunately, there isn't an obviously compelling argument for `$B$` and `$C$` to be dynamically calculated, the authors of the Mamba paper just note that allowing them to be dynamically computed yields better modeling performance.


<a class="citeend" key="kalman"></a>Kalman RE. (1960) A new approach to linear filtering and prediction problems. Transactions of the ASME--Journal of Basic Engineering, 82:35-45.

<script id='MathJax-script' async src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'></script>
<script defer src='https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/mathtex-script-type.min.js' integrity='sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT' crossorigin='anonymous'></script>
<script src="https://cdn.plot.ly/plotly-2.32.0.min.js" charset="utf-8"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">



<script src='../third_party/d3_.js'></script>
<script src="../third_party/tfjs.js"> </script>
<script src="../third_party/jqueryv3.6.1.js"></script>

<script src="ssm_impl_native.js"></script>
<script src="constants.js"></script>
<script src="visualize_dt_native.js"></script>
<script src="visualize_weights.js"></script>
<script src="visualize_weights_plotly.js"></script>
<script src="embeddings_over_time.js"></script>
<script src="init.js"></script>

<link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css"
      integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc"
      crossOrigin="anonymous"
    />
